{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee8861fa-15e4-44b3-b60b-4e381ed34b09",
   "metadata": {},
   "source": [
    "# 03 - Convolutional Neural Network Model Training Notebook\n",
    "Author: George Gorospe, george.gorospe@nmaia.net (updated 1/19/2024)\n",
    "\n",
    "# In this third notebook, we'll use the the data we previously collected tfo train a Convolutional Neural Network (CNN). \n",
    "\n",
    "Training a neural network results in a machine learning model. In this case the resulting model will serve as our pilot for our self-driving car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75e13514-020a-42aa-bfb3-ff77a3f51c9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-26T03:16:57.402529Z",
     "iopub.status.busy": "2024-01-26T03:16:57.401904Z",
     "iopub.status.idle": "2024-01-26T03:17:02.569305Z",
     "shell.execute_reply": "2024-01-26T03:17:02.568047Z",
     "shell.execute_reply.started": "2024-01-26T03:16:57.402482Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "### Machine Learning Libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Custom dataset object library\n",
    "from xy_dataset import XYDataset\n",
    "\n",
    "# General Libraries \n",
    "import cv2, glob, os, fnmatch\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ipyfilechooser import FileChooser\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "\n",
    "# Jupyter Laboratory Libraries\n",
    "import ipywidgets\n",
    "import traitlets\n",
    "from IPython.display import display\n",
    "\n",
    "# Nvidia library for images\n",
    "from jetcam.utils import bgr8_to_jpeg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42afe817-8dde-4144-9684-3e825c08f040",
   "metadata": {},
   "source": [
    "### Selecting a Dataset for Training\n",
    "Use the following folder chooser to select the folder where your dataset is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76153cb2-ee2b-4df1-ae57-253a65fc9256",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-26T03:17:02.572709Z",
     "iopub.status.busy": "2024-01-26T03:17:02.571280Z",
     "iopub.status.idle": "2024-01-26T03:17:02.620203Z",
     "shell.execute_reply": "2024-01-26T03:17:02.619085Z",
     "shell.execute_reply.started": "2024-01-26T03:17:02.572663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6490a22971f84add80399d392875f899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileChooser(path='/home/racer_core/Datasets', filename='', title='', show_hidden=False, select_desc='Select', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and display a FileChooser widget\n",
    "fc = FileChooser('/home/racer_core/Datasets')\n",
    "display(fc)\n",
    "fc.show_only_dirs = True\n",
    "# Change the title (use '' to hide)\n",
    "fc.title = '<b>Choose Dataset for Training</b>'\n",
    "\n",
    "# Sample callback function\n",
    "def change_title(chooser):\n",
    "    chooser.title = '<b>Directory Selected.</b>'\n",
    "\n",
    "# Register callback function\n",
    "fc.register_callback(change_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed46c925-1997-42e1-bced-49ad9d76faf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-26T03:17:09.860583Z",
     "iopub.status.busy": "2024-01-26T03:17:09.859528Z",
     "iopub.status.idle": "2024-01-26T03:17:09.905406Z",
     "shell.execute_reply": "2024-01-26T03:17:09.904235Z",
     "shell.execute_reply.started": "2024-01-26T03:17:09.860504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset found!\n",
      "Number of files found in datadset: 5012\n"
     ]
    }
   ],
   "source": [
    "# Inspecting Dataset\n",
    "\n",
    "# Output from file chooser\n",
    "DATASET_DIR = fc.selected_path\n",
    "dataset_folder_name = DATASET_DIR.split(\"/\")[-1]\n",
    "\n",
    "\n",
    "# Information about the dataset, number of data points and a listing of the data points.\n",
    "num_files =  len(glob.glob(os.path.join(DATASET_DIR, '*.jpg')))\n",
    "file_list = fnmatch.filter(os.listdir(DATASET_DIR), '*.jpg')\n",
    "if num_files > 0:\n",
    "    print(\"Dataset found!\")\n",
    "    print(\"Number of files found in datadset: \" + str(num_files))\n",
    "elif num_files == 0:\n",
    "  print(\"No data in selected directory, choose again?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "425b0e66-2ac7-454e-aac7-1b01645d8249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-26T03:17:10.265141Z",
     "iopub.status.busy": "2024-01-26T03:17:10.264343Z",
     "iopub.status.idle": "2024-01-26T03:17:10.340100Z",
     "shell.execute_reply": "2024-01-26T03:17:10.338654Z",
     "shell.execute_reply.started": "2024-01-26T03:17:10.265090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory exists.\n",
      "Number of files in datadset: 5012\n"
     ]
    }
   ],
   "source": [
    "# Creating our dataset object. This object parses the file names to get the labels for each datapoint\n",
    "\n",
    "# These transforms adjust the images prior to training to promote robust performance\n",
    "# Note: Some transforms are commented out they are example of possible transforms to use in the future\n",
    "TRANSFORMS = transforms.Compose([\n",
    "    #transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),  # Color Jitter #1\n",
    "    #transforms.ColorJitter(brightness=1.0, hue=.3), # Color Jitter #2\n",
    "    #transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)), # Gaussian Blur #1\n",
    "    transforms.GaussianBlur(kernel_size=(7), sigma=(0.8)),  # Gaussian Blue #2\n",
    "\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "Sample_Dataset = XYDataset(DATASET_DIR,TRANSFORMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09b14c00-6b78-47e2-9f29-1bdffc28ff44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-26T03:17:12.565579Z",
     "iopub.status.busy": "2024-01-26T03:17:12.564925Z",
     "iopub.status.idle": "2024-01-26T03:17:12.617470Z",
     "shell.execute_reply": "2024-01-26T03:17:12.615250Z",
     "shell.execute_reply.started": "2024-01-26T03:17:12.565533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Data Points in Training Dataset: 3759\n",
      "Number of Data Points in Evaluate Dataset: 1253\n"
     ]
    }
   ],
   "source": [
    "# Using sklearn to split dataset into training and evaluation subsets\n",
    "\n",
    "def train_val_dataset(dataset, val_split=0.25):\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "    datasets = {}\n",
    "    datasets['train'] = Subset(dataset, train_idx)\n",
    "    datasets['evaluate'] = Subset(dataset, val_idx)\n",
    "    return datasets\n",
    "\n",
    "# Both \"Train\" and \"Evaluate\" datasets are within the datasets list\n",
    "datasets = train_val_dataset(Sample_Dataset)\n",
    "print(f\"Number of Data Points in Training Dataset: {len(datasets['train'])}\")\n",
    "print(f\"Number of Data Points in Evaluate Dataset: {len(datasets['evaluate'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69cb4ae5-70a9-4c20-8a4b-f88009821bcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-26T03:17:13.563301Z",
     "iopub.status.busy": "2024-01-26T03:17:13.562154Z",
     "iopub.status.idle": "2024-01-26T03:17:13.574513Z",
     "shell.execute_reply": "2024-01-26T03:17:13.572103Z",
     "shell.execute_reply.started": "2024-01-26T03:17:13.563222Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the Dataloaders for both the 'train' and the 'eval' datasets\n",
    "# Here the datasets ('train' and 'evaluate') are input into DataLoaders\n",
    "# DataLoaders deliver the data to the training algorithm when requested.\n",
    "# They deliver the data in 'minibatches' , and reshuffle the data for each epoch\n",
    "train_dataloader = DataLoader(datasets['train'], batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(datasets['evaluate'], batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed829b-99ca-4a0b-b188-d7be901f23f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting an example of the data from the train loader\n",
    "\n",
    "# Create a figure for both images\n",
    "fig = plt.figure(figsize= (10, 10))\n",
    "\n",
    "# Create a subplot for the array of images\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "\n",
    "# Get an image, list of annotations and labels from the train dataloader\n",
    "train_image, ann, train_labels = next(iter(train_dataloader))\n",
    "\n",
    "# Getting the label for the image\n",
    "x = train_labels[0].numpy()[0]\n",
    "x = int(224 * (x / 2.0 + 0.5))\n",
    "\n",
    "# Reading the raw image from file\n",
    "file_path = ann['image_path'][0]\n",
    "print(f\"Selected File: {file_path}\")\n",
    "print(f\"Label value (x): {x}\")\n",
    "img = cv2.imread(file_path)\n",
    "\n",
    "# Plotting the raw image w/ label\n",
    "circ = Circle((x,112),15)\n",
    "ax.add_patch(circ)\n",
    "ax.imshow(img)\n",
    "\n",
    "# Getting the transformed image from the dataloader (all images on the\n",
    "train_image = train_image.numpy()[0]\n",
    "train_image = np.moveaxis(train_image, 0, -1)\n",
    "\n",
    "# Plotting the transformed image w/ label\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "circ = Circle((x,112),15)\n",
    "ax.add_patch(circ)\n",
    "ax.imshow(train_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1522cf92-3e0a-47e0-a327-c2485a9c9b1e",
   "metadata": {},
   "source": [
    "## Training the CNN on the Selected Dataset\n",
    "Next, we'll setup the training algorithm for our machine learning model.\n",
    "As we prepare to train our model we need to make choices about the way we'll train it.\n",
    "These choices can impact how long it takes to train the model and the overall accuracy of the model.\n",
    "\n",
    "The user-set parameters of the training algorithm are often called \"Hyper-Parameters\"\n",
    "You can set your hyper parameters below, make sure to track which setting you used for your training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05e9082-26d8-4a69-a472-1aac5657fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Training Hyper Parameters:\n",
    "\n",
    "########## [ACTION REQUIRED] Set name for new machine learning model #################\n",
    "model_file_name = \"large_model2.pth\"\n",
    "training_notes = \"A study of large models, resnet50 w/ frozen layers.\"\n",
    "\n",
    "\n",
    "# Number of training epochs:\n",
    "epochs = 2\n",
    "\n",
    "# Data loader batch size:\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Model Output\n",
    "output_dim = 2\n",
    "\n",
    "######### Select a Machine learning model structure (Neural Network) ###########\n",
    "######### Uncomment both the model and the fully connected layer \"model.fc\"\n",
    "\n",
    "# Resnet 18\n",
    "#model = torchvision.models.resnet18(pretrained=True)\n",
    "#model.fc = torch.nn.Linear(512, output_dim)\n",
    "\n",
    "# Resnet 34\n",
    "#model = torchvision.models.resnet34(pretrained=True)\n",
    "#model.fc = torch.nn.Linear(512, output_dim)\n",
    "\n",
    "# Resnet 50\n",
    "#model = torch.hub.load(\"pytorch/vision\", \"resnet50\", weights=\"IMAGENET1K_V2\")\n",
    "#model.fc = torch.nn.Linear(2048, output_dim)\n",
    "\n",
    "# MobileNet V2\n",
    "model = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "model.fc = torch.nn.Linear(2048, output_dim)\n",
    "\n",
    "# MobileNet V3\n",
    "#model = torchvision.models.mobilenet_v3_large(pretrained=True)\n",
    "# model.classifier[-1] = torch.nn.Linear(4096, output_dim)\n",
    "\n",
    "# ALEXNET\n",
    "# model = torchvision.models.alexnet(pretrained=True)\n",
    "# model.classifier[-1] = torch.nn.Linear(4096, output_dim)\n",
    "\n",
    "# SQUEEZENET \n",
    "# model = torchvision.models.squeezenet1_1(pretrained=True)\n",
    "# model.classifier[1] = torch.nn.Conv2d(512, output_dim, kernel_size=1)\n",
    "# model.num_classes = len(dataset.categories)\n",
    "\n",
    "# DENSENET 121\n",
    "# model = torchvision.models.densenet121(pretrained=True)\n",
    "# model.classifier = torch.nn.Linear(model.num_features, output_dim)\n",
    "\n",
    "\n",
    "# If you wanted to train fewer of the layers (freeze some layers)\n",
    "#Freeze all of the weights in ResNet18\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = False\n",
    "\n",
    "# Adding a fully connected layer to the top/head of the model\n",
    "\n",
    "# Model optimizer:\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "#Loading a GPU if available and otherwise a CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ed861-d2d7-491e-a288-44f2cbdeac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_and_evaluation(epochs):\n",
    "  # Training Timing\n",
    "  start_time = datetime.now()\n",
    "\n",
    "  ############# Initiating Training Process ##############\n",
    "  # First set model to train mode\n",
    "  model.train()\n",
    "\n",
    "  print(\"Starting training process ...\")\n",
    "  # Start training process dependent on number of epochs\n",
    "  while epochs > 0:\n",
    "    print(\"######### Epoch: \" + str(epochs) + \" #########\")\n",
    "    # Index\n",
    "    i = 0\n",
    "    sum_loss = 0.0\n",
    "    error_count = 0.0\n",
    "\n",
    "    # Training Loop\n",
    "    # Process each batch of data points in the train loader\n",
    "    for images, category_idx, xy in iter(train_dataloader):\n",
    "      # send data to device\n",
    "      images = images.to(device)\n",
    "      xy = xy.to(device)\n",
    "\n",
    "      # zero gradients of parameters\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # execute model to get outputs\n",
    "      outputs = model(images)\n",
    "\n",
    "      # compute MSE loss over x, y coordinates for associated categories\n",
    "      loss = 0.0\n",
    "      loss += torch.mean((outputs - xy)**2)\n",
    "      #for batch_idx, cat_idx in enumerate(list(category_idx.flatten())):\n",
    "      #    loss += torch.mean((outputs[batch_idx][2 * cat_idx:2 * cat_idx+2] - xy[batch_idx])**2)\n",
    "      #loss /= len(category_idx)\n",
    "\n",
    "      # run backpropogation to accumulate gradients\n",
    "      loss.backward()\n",
    "\n",
    "      # step optimizer to adjust parameters\n",
    "      optimizer.step()\n",
    "\n",
    "      # increment progress\n",
    "      # NO TRAINING ACCURACY: no correct answer for regression, only loss\n",
    "      #count = len(category_idx.flatten())\n",
    "      #i += count\n",
    "      i += len(xy)\n",
    "      sum_loss += float(loss)\n",
    "      #progress_widget.value = i / len(dataset)\n",
    "      #loss_widget.value = sum_loss / i\n",
    "\n",
    "\n",
    "      print(\"Mean Square Error (MSE): \" + str(sum_loss/i))\n",
    "\n",
    "    #sum_loss.append(totalLoss)\n",
    "    #print(f\"Training Accuracy: {testAccuracy / len(training)}\")\n",
    "\n",
    "\n",
    "    # Evaluation Loop\n",
    "    i = 0\n",
    "    evaluation_loss = 0.0\n",
    "    for images, category_idx, xy in test_dataloader:\n",
    "\n",
    "        # Put the model into evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # send data to device\n",
    "        images = images.to(device)\n",
    "        xy = xy.to(device)\n",
    "\n",
    "        # execute model to get outputs\n",
    "        outputs = model(images)\n",
    "\n",
    "        # compute MSE loss over x, y coordinates for associated categories\n",
    "        loss = 0.0\n",
    "        loss += torch.mean((outputs - xy)**2)\n",
    "        i += len(xy)\n",
    "        evaluation_loss += float(loss)\n",
    "    print(f\"Validation Test Mean Square Error (MSE): {evaluation_loss / i}\")\n",
    "    #Save our model for each epoch\n",
    "    #torch.save(model.state_dict(), file)\n",
    "\n",
    "    # End of the current epoch\n",
    "    epochs = epochs -1\n",
    "  end_time = datetime.now()\n",
    "\n",
    "  # get the execution time\n",
    "  elapsed_time = end_time - start_time\n",
    "  training_duration_time_formatted = str(elapsed_time)\n",
    "  print('Execution time:', training_duration_time_formatted)\n",
    "\n",
    "  # Writing training details to training log\n",
    "  f = open(\"/home/racer_core/Models/training_log.txt\", \"a\")\n",
    "  f.write(\"\\n\")\n",
    "  dt_string = start_time.strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "  f.write(f\"Training Report: {dt_string} \\n\")\n",
    "  f.write(f\"Output Model File: {model_file_name}\\n\")\n",
    "  f.write(f\"Selected Dataset: {dataset_folder_name}, Number Data Points: {num_files}\\n\")\n",
    "  f.write(f\"Model: {model.__class__.__name__}, Epochs: {epochs}, Batch Size: {BATCH_SIZE}\\n\")\n",
    "  f.write(f\"Training Notes: {training_notes}\\n\")\n",
    "  f.write(f\"Transforms used in this training: {TRANSFORMS}\\n\")\n",
    "  f.write(f\"Final model evaluation loss: {evaluation_loss/i}\\n\")\n",
    "  f.write(f\"Total training & evaluation time: {training_duration_time_formatted}\\n\")\n",
    "  f.write(\"\\n\")\n",
    "  f.close()\n",
    "\n",
    "  return model #trainLoss, validationLoss, model\n",
    "\n",
    "# START TRAINING\n",
    "model = training_and_evaluation(epochs)\n",
    "\n",
    "# SAVE TGHE MODEL TO FILE\n",
    "model_folder = \"/home/racer_core/Models/\"\n",
    "model_file_path = model_folder + model_file_name\n",
    "torch.save(model.state_dict(), model_file_path)\n",
    "print(f\"Saved new model as: {model_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67c3464-96c2-43a3-b5cd-94364c76a3c3",
   "metadata": {},
   "source": [
    "### Optimizing the Machine Learning Model to Run on the Robot\n",
    "In this final step to the training process, we'll optimize the model.\n",
    "We optimize the model so that it will run as fast as our camera collects data.\n",
    "\n",
    "Optimization involves two steps:\n",
    "1. Export the trained machine learning model from a pytorch model (.pth) to a general and open format called ONNX (.onnx). ONNX stands for Open Neural Network Exchange.\n",
    "2. Convert the ONNX model to a version designed for fast operation, a tensorRT model (.TRT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49004919-54a9-4178-b496-dc9f259e970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### STEP 1: Conversion of the Pytorch model to ONNX format\n",
    "\n",
    "import torch.onnx\n",
    "\n",
    "# Retrieve an example of the input, an image, from the data loader.\n",
    "train_image, ann, train_labels = next(iter(train_dataloader))\n",
    "train_image = train_image.to(device) # Need to ensure that the example input is on the device\n",
    "\n",
    "onnx_model_file_name = model_file_name.split(\".\")[0] + \".onnx\"\n",
    "onnx_model_file_path = \"/home/racer_core/Models/onnx/\" + onnx_model_file_name\n",
    "\n",
    "onnx_model = torch.onnx.export(model, train_image , onnx_model_file_path, verbose=False)\n",
    "print(f\"New ONNX model file saved to: {onnx_model_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c70e64-ca60-4350-bacb-45449f61135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the recently created onnx \n",
    "import onnx\n",
    "onnx_model = onnx.load(onnx_model_file_path)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610be6b5-dddc-438e-950f-e3953c16d065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pycuda.driver as cuda\n",
    "#import pycuda.autoinit\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    " \n",
    "# logger to capture errors, warnings, and other information during the build and inference phases\n",
    "TRT_LOGGER = trt.Logger()\n",
    " \n",
    "def build_engine(onnx_file_path):\n",
    "    # initialize TensorRT engine and parse ONNX model\n",
    "    builder = trt.Builder(TRT_LOGGER)\n",
    "    network = builder.create_network()\n",
    "    parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "     \n",
    "    # parse ONNX\n",
    "    with open(onnx_file_path, 'rb') as model:\n",
    "        print('Beginning ONNX file parsing')\n",
    "        parser.parse(model.read())\n",
    "    print('Completed parsing of ONNX file')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
