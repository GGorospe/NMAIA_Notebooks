{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee8861fa-15e4-44b3-b60b-4e381ed34b09",
   "metadata": {},
   "source": [
    "### 03 - Convolutional Neural Network Model Training Notebook\n",
    "Author: George Gorospe, george.gorospe@nmaia.net (updated 1/3/2026)\n",
    "\n",
    "# In this third notebook, we'll use the the data we previously collected to train an AI Pilot for our racer. \n",
    "\n",
    "Technically, we're using the data to train a convolutional neural network. This network uses convolutions, or special filters that are adjusted to promote the part of the image useful for driving. The training process adjusts the filters repeatedly until the model accurately drives just like you did when you drove the racer manually. In the next notebook, we'll use the model as our AI pilot for our self-driving car.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e50229f-a608-4bf0-ba98-099b266072f1",
   "metadata": {},
   "source": [
    "<font color='red' size='6'>IMPORTANT: Use the AC adaptor (wall power) when running this notebook.</font>\n",
    "\n",
    "Training our machine learning model is power intensive, the robot's battery can't supply enough current for the process. This occasionally causes the computer to shutdown during training. For best results, always use wall power during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e13514-020a-42aa-bfb3-ff77a3f51c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "### Machine Learning Libraries\n",
    "import torch # Import the PyTorch library\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "import torchvision # Import the TorchVision library from PyTorch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# IPython Libraries for display and widgets\n",
    "import traitlets\n",
    "import ipywidgets\n",
    "import ipywidgets.widgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, Layout\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "\n",
    "# Custom dataset object library\n",
    "from xy_dataset import XYDataset\n",
    "\n",
    "# General Libraries \n",
    "import cv2, glob, os, fnmatch, collections, random\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "import numpy as np\n",
    "from ipyfilechooser import FileChooser\n",
    "\n",
    "\n",
    "# Nvidia library for images\n",
    "from jetcam.utils import bgr8_to_jpeg\n",
    "\n",
    "# Custom plot function\n",
    "def live_plot(data_dict, model_file_name=\"model\", figsize=(7,5), title='Model Error Chart:'):\n",
    "    clear_output(wait=True)\n",
    "    fig1 = plt.figure(figsize=figsize)\n",
    "    for label,data in data_dict.items():\n",
    "        plt.plot(data, label= model_file_name + \".pth\") #label)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('Model Loss x1000')\n",
    "    plt.legend(loc='upper right') # the plot evolves to the right\n",
    "    plt.show();\n",
    "    fig1.savefig(training_chart_file_path)\n",
    "\n",
    "# Data collection for MSE information\n",
    "data = collections.defaultdict(list)\n",
    "\n",
    "\n",
    "# Empty the cuda cache from previous runs, helps to free memory  \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42afe817-8dde-4144-9684-3e825c08f040",
   "metadata": {},
   "source": [
    "### Selecting a Dataset for Training\n",
    "Use the following folder chooser to select the folder where your dataset is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76153cb2-ee2b-4df1-ae57-253a65fc9256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display a FileChooser widget\n",
    "fc = FileChooser('/home/student/Datasets')\n",
    "display(fc)\n",
    "fc.show_only_dirs = True\n",
    "# Change the title (use '' to hide)\n",
    "fc.title = '<b>Choose Dataset for Training</b>'\n",
    "\n",
    "# Sample callback function\n",
    "def change_title(chooser):\n",
    "    chooser.title = '<b>Directory Selected.</b>'\n",
    "\n",
    "# Register callback function\n",
    "fc.register_callback(change_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed46c925-1997-42e1-bced-49ad9d76faf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting Dataset\n",
    "\n",
    "# Output from file chooser\n",
    "DATASET_DIR = fc.selected_path\n",
    "dataset_folder_name = DATASET_DIR.split(\"/\")[-1]\n",
    "\n",
    "\n",
    "# Information about the dataset, number of data points and a listing of the data points.\n",
    "num_files =  len(glob.glob(os.path.join(DATASET_DIR, '*.jpg')))\n",
    "file_list = fnmatch.filter(os.listdir(DATASET_DIR), '*.jpg')\n",
    "if num_files > 0:\n",
    "    print(\"Dataset found!\")\n",
    "    print(\"Number of files found in datadset: \" + str(num_files))\n",
    "elif num_files == 0:\n",
    "  print(\"No data in selected directory, choose again?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b0e66-2ac7-454e-aac7-1b01645d8249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our dataset object. This object parses the file names to get the labels for each datapoint\n",
    "\n",
    "# These transforms adjust the images prior to training to promote robust performance\n",
    "# Note: Some transforms are commented out they are example of possible transforms to use in the future\n",
    "TRANSFORMS = transforms.Compose([\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),  # Color Jitter #1\n",
    "    #transforms.ColorJitter(brightness=1.3, hue=.3), # Color Jitter #2\n",
    "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)), # Gaussian Blur #1\n",
    "    #transforms.GaussianBlur(kernel_size=(7), sigma=(0.8)),  # Gaussian Blur #2\n",
    "\n",
    "    transforms.Resize((224, 224)), # MUST USE\n",
    "    transforms.ToTensor(), # MUST USE\n",
    "    transforms.Lambda(lambda x: x[[2,1,0], ...]), # MUST USE\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # MUST USE\n",
    "])\n",
    "\n",
    "Sample_Dataset = XYDataset(DATASET_DIR,TRANSFORMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b14c00-6b78-47e2-9f29-1bdffc28ff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn to split dataset into training and evaluation subsets\n",
    "\n",
    "def train_val_dataset(dataset, val_split=0.20):\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "    datasets = {}\n",
    "    datasets['train'] = Subset(dataset, train_idx)\n",
    "    datasets['evaluate'] = Subset(dataset, val_idx)\n",
    "    return datasets\n",
    "\n",
    "# Both \"Train\" and \"Evaluate\" datasets are within the datasets list\n",
    "datasets = train_val_dataset(Sample_Dataset)\n",
    "print(f\"Number of Data Points in Training Dataset: {len(datasets['train'])}\")\n",
    "print(f\"Number of Data Points in Evaluate Dataset: {len(datasets['evaluate'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cb4ae5-70a9-4c20-8a4b-f88009821bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Dataloaders for both the 'train' and the 'eval' datasets\n",
    "# Here the datasets ('train' and 'evaluate') are input into DataLoaders\n",
    "# DataLoaders deliver the data to the training algorithm when requested.\n",
    "# They deliver the data in 'minibatches' , and reshuffle the data for each epoch\n",
    "BATCH_SIZE = 32 # DEFAULT = 32, if you get a 'malloc' error during training, try reducing this to 16\n",
    "NUM_WORKERS = 2 # the number of CPU cores to use in supplying the next batch\n",
    "\n",
    "train_dataloader = DataLoader(datasets['train'], \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              shuffle=True,\n",
    "                              num_workers=NUM_WORKERS,\n",
    "                              pin_memory=True\n",
    "                             )\n",
    "test_dataloader = DataLoader(datasets['evaluate'], \n",
    "                             batch_size=BATCH_SIZE, \n",
    "                             shuffle=True,\n",
    "                             num_workers=NUM_WORKERS,\n",
    "                             pin_memory=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae19793c-5d3c-4fb4-a62e-edb70edcfaaa",
   "metadata": {},
   "source": [
    "## Next, we'll visualize a of the datapoint from our dataset. Run several times to see more datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1098922e-ade6-4d14-b059-f2627a7baae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Verification of datpoints by visualzation\n",
    "# Use this cell to visualize several of your data points.\n",
    "# The imags are displayed with a green dot marking the label location.\n",
    "# Question: do the labels make sense? Are they where you expect them to be?\n",
    "# If yes, then you can go ahead and perform the training.\n",
    "# If not, then considering collecting data again. Focus on careful, slow driving in the center of the lane.\n",
    "\n",
    "\n",
    "# 1. Define the transform for human eye verification (BGR -> RGB, No Normalization)\n",
    "verification_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x[[2, 1, 0], ...]) \n",
    "])\n",
    "\n",
    "# 2. Load the dataset into the special verify_dataset we'll use just to view the datapoints\n",
    "verify_dataset = XYDataset(DATASET_DIR, verification_transform)\n",
    "\n",
    "# 3. Get an image and its label selected at random from the dataset\n",
    "# verify_dataset[index] returns (image, category_index, xy_value)\n",
    "index = random.randint(0, num_files-1)\n",
    "image_tensor, ann, xy = verify_dataset[index]\n",
    "\n",
    "# 4. Prepare Image for Plotting\n",
    "image_to_plot = image_tensor.permute(1, 2, 0).numpy()\n",
    "\n",
    "# 5. Prepare Label (Calculate Pixel Coordinates)\n",
    "# We assume the label x is normalized [-1, 1]. We map it to [0, 224].\n",
    "# xy is a tensor, usually [x, y]. We take the first element for x.\n",
    "if isinstance(xy, torch.Tensor):\n",
    "    raw_x = xy[0].item()\n",
    "else:\n",
    "    raw_x = xy[0]\n",
    "\n",
    "# Formula: pixel = width * (normalized_val / 2.0 + 0.5)\n",
    "\n",
    "pixel_x = int(224 * (raw_x / 2.0 + 0.5))\n",
    "pixel_y = 112 # We assume a fixed center height for the lane marker (half of 224)\n",
    "\n",
    "# 6. Plot Image with Label\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.imshow(image_to_plot)\n",
    "\n",
    "# Draw a Green Circle at the target location\n",
    "circ = Circle((pixel_x, pixel_y), 5, color='lime', fill=True, linewidth=3)\n",
    "ax.add_patch(circ)\n",
    "\n",
    "ax.set_title(f\"Data Point Verification\")\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1522cf92-3e0a-47e0-a327-c2485a9c9b1e",
   "metadata": {},
   "source": [
    "## Training the CNN on the Selected Dataset\n",
    "Next, we'll setup the training algorithm for our machine learning model.\n",
    "As we prepare to train our model we need to make choices about the way we'll train it.\n",
    "These choices can impact how long it takes to train the model and the overall accuracy of the model.\n",
    "\n",
    "The user-set parameters of the training algorithm are often called \"Hyper-Parameters\"\n",
    "You can set your hyper parameters below, make sure to track which setting you used for your training!\n",
    "## [ACTION REQUIRED] change the name for your machine learning model, \"choose_a_new_name_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05e9082-26d8-4a69-a472-1aac5657fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Training Hyper Parameters:\n",
    "\n",
    "########## [ACTION REQUIRED] Set name for new machine learning model #################\n",
    "model_file_name = \"choose_a_new_name_model\" # Change this, leave the \"\"\n",
    "training_notes = \"write a good description of the model you're training\" # leave the \"\"\n",
    "# Example training notes: testing new dataset collected on Friday 1/10/25 at the library\n",
    "\n",
    "# Number of training epochs: (15 to 25 is a great range to start with)\n",
    "# epochs are training cycles\n",
    "epochs =  25 # Hyperparameter (has a direct effect on the accuracy of the model)\n",
    "\n",
    "# Models have many layers, as you saw above. We can train all layers or just the final layer.\n",
    "#   train_all_layers = True --> Training takes longer, but may produce a better model. Do this if you have time to train\n",
    "#   train_all_layers = False --> Do this first, takes less time. Try out the model, if it already has good accuracy, try training all layers next.\n",
    "train_all_layers = False\n",
    "\n",
    "#####################################################################################\n",
    "model_folder = \"/home/student/Models/\"\n",
    "model_file_path = model_folder + model_file_name + \".pth\"\n",
    "training_chart_file_path = model_folder+model_file_name+\".png\"\n",
    "\n",
    "# Model Name check\n",
    "if os.path.isfile(model_file_path):\n",
    "    raise Exception('Sorry, model with same name already exists, choose a different model file name.')\n",
    "\n",
    "# Model Output\n",
    "output_dim = 2\n",
    "# Total number of epochs\n",
    "total_epochs = epochs\n",
    "\n",
    "######### Select a Machine learning model structure (Neural Network) ###########\n",
    "######### Uncomment both the model and the fully connected layer \"model.fc\"\n",
    "\n",
    "# Resnet 18\n",
    "model_name = \"Resnet 18\"\n",
    "weights = ResNet18_Weights.DEFAULT \n",
    "model = torchvision.models.resnet18(weights=weights)\n",
    "\n",
    "\n",
    "# Resnet 34\n",
    "# model_name = \"Resnet 34\"\n",
    "# model = torchvision.models.resnet34(pretrained=True)\n",
    "\n",
    "# Resnet 50\n",
    "# model_name = \"Resnet 50\"\n",
    "# model = torch.hub.load(\"pytorch/vision\", \"resnet50\", weights=\"IMAGENET1K_V2\")\n",
    "\n",
    "# MobileNet V2\n",
    "#model_name = \"MobileNet_V2\"\n",
    "#model = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "#model.fc = torch.nn.Linear(2048, output_dim)\n",
    "\n",
    "# MobileNet V3\n",
    "#model_name = \"MobileNet_V3\"\n",
    "#model = torchvision.models.mobilenet_v3_large(pretrained=True)\n",
    "#model.classifier[-1] = torch.nn.Linear(1280, output_dim)\n",
    "\n",
    "# ALEXNET\n",
    "#model_name = \"ALEXNET\"\n",
    "#model = torchvision.models.alexnet(pretrained=True)\n",
    "#model.classifier[-1] = torch.nn.Linear(4096, output_dim)\n",
    "\n",
    "# Save the model structure to use later during optimization\n",
    "model_structure = model\n",
    "\n",
    "# Control which part of the model is trained, all layers or just the final layer.\n",
    "if train_all_layers == False:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Adding a fully connected layer to the top/head of the model\n",
    "model.fc = torch.nn.Linear(512, output_dim) # Use 512 for ResNet 18 and 34, Use 2048 for ResNet50 models\n",
    "\n",
    "# Model optimizer:\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "#Loading a GPU if available and otherwise a CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d492b19a-208f-4658-9100-82a6ecf1b3e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def training_and_evaluation(epochs):\n",
    "    # Setting up a simple user interface for the training process\n",
    "    epoch_index_display = widgets.IntText(value=epochs, description='Epoch: ', disabled=False)\n",
    "    epoch_progress_display = widgets.FloatProgress(value=0.0,max=len(datasets['train']) // BATCH_SIZE +1 , description='Epoch Progress:', bar_style='info', style={'bar_color': '#808080'}, orientation='horizontal')\n",
    "    training_error_display = widgets.FloatText(value=0.0, description='Mean Square Error (MSE):', disabled=False) \n",
    "    progress_meter = widgets.FloatProgress(value=0.0, min=0, max=total_epochs, description='Progress:', bar_style='info', style={'bar_color': '#40E0D0'}, orientation='horizontal')\n",
    "    system_status = widgets.Text(value='system_status', placeholder='system_status', description='Status:',disabled=False)\n",
    "    epoch_display = widgets.HBox([epoch_index_display, epoch_progress_display])\n",
    "    training_display = widgets.HBox([training_error_display, progress_meter])\n",
    "    display(epoch_display)\n",
    "    display(training_display)\n",
    "    display(system_status)\n",
    "    out1 = widgets.Output()\n",
    "\n",
    "    display(widgets.HBox([out1]))\n",
    "\n",
    "\n",
    "    # Training Timing\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # Writing training details to training log\n",
    "    f = open(\"/home/student/Models/training_log.txt\", \"a\")\n",
    "    f.write(\"\\n\")\n",
    "    dt_string = start_time.strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "    f.write(f\"Training Report: {dt_string} \\n\")\n",
    "    f.write(f\"Output Model File: {model_file_name}\\n\")\n",
    "    f.write(f\"Selected Dataset: {dataset_folder_name}, Number Data Points: {num_files}\\n\")\n",
    "    f.write(f\"Model: {model_name}, Epochs: {epochs}, Batch Size: {BATCH_SIZE}\\n\")\n",
    "    f.write(f\"Training Notes: {training_notes}\\n\")\n",
    "    \n",
    "    ############# Initiating Training Process ##############\n",
    "\n",
    "\n",
    "    system_status.value = \"Starting training process ...\"\n",
    "\n",
    "    # Enable the cuDNN autotuner which looks for the most efficient algorithms for our training process\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Start training process dependent on number of epochs\n",
    "    # EPOCH LOOP: \n",
    "    while epochs > 0:\n",
    "        system_status.value = \"Training ...\"\n",
    "        # Index\n",
    "        i = 0\n",
    "        sum_loss = 0.0\n",
    "        error_count = 0.0\n",
    "\n",
    "        # Training phase: batch loop\n",
    "        # First set model to train mode\n",
    "        model.train()\n",
    "        # BATCH LOOP: Process each batch of data points in the train loader\n",
    "        for images, category_idx, xy in iter(train_dataloader):\n",
    "            i = i + 1\n",
    "\n",
    "            # send data to device\n",
    "            images = images.to(device)\n",
    "            xy = xy.to(device)\n",
    "            \n",
    "            # zero gradients of parameters\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # execute model to get outputs\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # run backpropogation to accumulate gradients\n",
    "            loss = 0.0\n",
    "            loss += torch.mean((outputs - xy)**2)\n",
    "            loss.backward()\n",
    "            \n",
    "            # step optimizer to adjust parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "\n",
    "            # Update Epoch progress meter:\n",
    "            if i % 10 == 0: # update every 20 baches\n",
    "                epoch_progress_display.value = i\n",
    "            \n",
    "            # compute MSE loss over x, y coordinates for associated categories\n",
    "            test = False\n",
    "            if test == True:\n",
    "                    \n",
    "                xy = xy.cpu()\n",
    "                outputs = outputs.detach().cpu().numpy().flatten()\n",
    "                \n",
    "                MSE = 0.0\n",
    "                for j in range(len(xy)):\n",
    "                  x = (224*(xy[j].numpy()[0]/2.0 + 0.5))\n",
    "                  xi = (224*(outputs[j]/2.0 + 0.5))\n",
    "                  MSE = MSE + (x-xi)**2\n",
    "                \n",
    "                MSE = MSE/len(xy)\n",
    "                #training_error_display.value = MSE\n",
    "    \n",
    "        scheduler.step()\n",
    "\n",
    "        # Evaluation phase: Evaluation loop\n",
    "        # Evaluation Loop\n",
    "        system_status.value = \"Evaluating ...\"\n",
    "        # Put the model into evaluation mode\n",
    "        model.eval()\n",
    "        evaluation_loss = 0.0\n",
    "        i = 0\n",
    "        with torch.no_grad():\n",
    "            for images, category_idx, xy in test_dataloader: # TODO: Make sure that xy is not a tensor\n",
    "            \n",
    "\n",
    "            \n",
    "                # send data to device\n",
    "                images = images.to(device)\n",
    "                xy = xy.to(device)\n",
    "            \n",
    "                # execute model to get outputs\n",
    "                outputs = model(images)\n",
    "        \n",
    "                loss = torch.mean((outputs - xy)**2)\n",
    "                i += len(xy)\n",
    "                evaluation_loss += loss.item()*1000\n",
    "\n",
    "        \n",
    "        #MSE = MSE/len(xy)\n",
    "        #evaluation_loss = MSE\n",
    "        print(f\"Epoch: {epochs}, Evaluation Loss: {evaluation_loss/i}\")\n",
    "        training_error_display.value = evaluation_loss/i\n",
    "\n",
    "        # Update Plot\n",
    "        with out1:\n",
    "            data['MSE'].append(evaluation_loss/i)\n",
    "            live_plot(data, model_file_name=model_file_name, title='Model Error Chart: ' + model_file_name + \".pth\")\n",
    "            \n",
    "        # End of the current epoch\n",
    "        epochs = epochs -1\n",
    "        epoch_index_display.value = epochs\n",
    "        progress_meter.value = (total_epochs - epochs)\n",
    "\n",
    "    \n",
    "    # get the execution time\n",
    "    end_time = datetime.now()\n",
    "    elapsed_time = end_time - start_time\n",
    "    training_duration_time_formatted = str(elapsed_time)\n",
    "    print('Execution time:', training_duration_time_formatted)\n",
    "    \n",
    "    # Finish writing to model training log\n",
    "    f.write(f\"Final model evaluation loss: {evaluation_loss/i}\\n\")\n",
    "    f.write(f\"Total training & evaluation time: {training_duration_time_formatted}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.close()\n",
    "    system_status.value = \"Training complete.\"   \n",
    "    return model #trainLoss, validationLoss, model\n",
    "\n",
    "# START TRAINING\n",
    "model = training_and_evaluation(epochs)\n",
    "\n",
    "# SAVE THE MODEL TO FILE\n",
    "torch.save(model.state_dict(), model_file_path)\n",
    "print(f\"Saved new model as: {model_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acafb67-a7dc-4a01-8be6-b56d0e267154",
   "metadata": {},
   "source": [
    "## Visualizing our new model's predictions\n",
    "### Our model uses what it learned from the data to infer or predict what the desired turning angle should be for new situations.\n",
    "### Run the next cell to see the accuracy of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af003036-9c7a-4a06-900a-3a7accb04571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_predictions(model, dataloader, num_images=5):\n",
    "    # 1. Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # 2. Get one batch of data\n",
    "    try:\n",
    "        images, _, xy = next(iter(dataloader))\n",
    "    except StopIteration:\n",
    "        print(\"DataLoader is empty.\")\n",
    "        return\n",
    "\n",
    "    # 3. Send to Device\n",
    "    images = images.to(device)\n",
    "    \n",
    "    # 4. Run Inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        \n",
    "    # 5. Move data back to CPU\n",
    "    images = images.cpu()\n",
    "    xy = xy.cpu()\n",
    "    outputs = outputs.cpu()\n",
    "    \n",
    "    # 6. Setup Plot\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(20, 5))\n",
    "    \n",
    "    # Un-normalization constants (ImageNet defaults)\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "    print(\"Visualization Legend:\")\n",
    "    print(\"---------------------\")\n",
    "    print(\"Green Circle = Ground Truth (Manual Label)\")\n",
    "    print(\"Red X       = Model Prediction\")\n",
    "    print(\"Values displayed are in the new 0-1000 range.\\n\")\n",
    "\n",
    "    for i in range(num_images):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # A. Un-normalize the image so it looks like a photo\n",
    "        img_tensor = images[i] * std + mean\n",
    "        img_np = torch.clamp(img_tensor, 0, 1).permute(1, 2, 0).numpy()\n",
    "        \n",
    "        ax.imshow(img_np)\n",
    "        \n",
    "        # B. Get Normalized Values (-1 to 1)\n",
    "        norm_true_x = xy[i][0].item()\n",
    "        norm_pred_x = outputs[i][0].item()\n",
    "        \n",
    "        # C. Calculate Display Values (Range 0-1000)\n",
    "        # This tells you the actual value in your new coordinate system\n",
    "        val_true_1000 = int((norm_true_x / 2.0 + 0.5) * 1000)\n",
    "        val_pred_1000 = int((norm_pred_x / 2.0 + 0.5) * 1000)\n",
    "        \n",
    "        # D. Calculate Pixel Coordinates (Range 0-224)\n",
    "        # We must scale this to 224 so it lands on the image correctly\n",
    "        pixel_true_x = (norm_true_x / 2.0 + 0.5) * 224\n",
    "        pixel_pred_x = (norm_pred_x / 2.0 + 0.5) * 224\n",
    "        \n",
    "        # Assume Y is centered for visualization (or use xy[i][1] if Y is trained)\n",
    "        pixel_y = 112 \n",
    "\n",
    "        # E. Draw Markers\n",
    "        # Ground Truth (Green Circle)\n",
    "        ax.add_patch(plt.Circle((pixel_true_x, pixel_y), 8, color='lime', fill=False, linewidth=3))\n",
    "        # Prediction (Red X)\n",
    "        ax.plot(pixel_pred_x, pixel_y, 'rx', markersize=15, markeredgewidth=3)\n",
    "        \n",
    "        # Title with 0-1000 values\n",
    "        ax.set_title(f\"True: {val_true_1000} | Pred: {val_pred_1000}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Run the sanity check\n",
    "visualize_model_predictions(model, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67c3464-96c2-43a3-b5cd-94364c76a3c3",
   "metadata": {},
   "source": [
    "### Optimizing the Machine Learning Model to Run on the Robot\n",
    "In this final step to the training process, we'll optimize the model.\n",
    "We optimize the model so that it will run as fast as our camera collects data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e7210b-7184-4630-ad19-9703dc099cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path = model_folder + model_file_name + \".pth\"\n",
    "\n",
    "# First Optimization Cell\n",
    "# Warm starting the new model to be optimized - loading weights from trained model into untrained model\n",
    "model = model_structure # this is the shape of the model before training\n",
    "model = model.cuda().eval().half()\n",
    "model.load_state_dict(torch.load(model_file_path, weights_only=True))\n",
    "\n",
    "# When executed, we should see, \"<All keys matched successfully>\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b98a3bf-dbe9-4ae3-a484-2cf5dd8becae",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Optimization of the Network ############\n",
    "### This step can take a few minutes or longer depending on the size of the mdoel\n",
    "\n",
    "# Custom library from Nvidia to accelerate inference\n",
    "from torch2trt import torch2trt\n",
    "\n",
    "# Example structure of the input data\n",
    "data = torch.zeros((1, 3, 224, 224)).cuda().half()\n",
    "\n",
    "# Model optimization via quantitization, or the reduction of overall model size by reducing the representation of model weights.\n",
    "model_trt = torch2trt(model, [data], fp16_mode=True)\n",
    "\n",
    "# Saving our new optimized model to disk\n",
    "optimized_model_folder = \"/home/student/Models/trt/\"\n",
    "optimized_model_file_path = optimized_model_folder + model_file_name + \"_TRT.pth\"\n",
    "torch.save(model_trt.state_dict(), optimized_model_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159e59f6-8417-4550-a8f2-0bfd3c4d320a",
   "metadata": {},
   "source": [
    "# NOTES: If the kernel quits during optimization of the network\n",
    "1. take note of the model name and structure (example: resnet 18)\n",
    "2. Go to U2: Model Optimization notebook and select your model for optimization\n",
    "3. Run all cells in U2: Model Optimization notebook.\n",
    "4. If this process fails reboot computer and go directly to U2 notebook to try again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e546c7d-f394-4471-af52-7a708046552b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
