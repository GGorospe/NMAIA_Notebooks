{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee8861fa-15e4-44b3-b60b-4e381ed34b09",
   "metadata": {},
   "source": [
    "# 03 - Convolutional Neural Network Model Training Notebook\n",
    "Author: George Gorospe, george.gorospe@nmaia.net (updated 4/15/2024)\n",
    "\n",
    "# In this third notebook, we'll use the the data we previously collected to train a Convolutional Neural Network (CNN). \n",
    "\n",
    "Training a neural network results in a machine learning model. In this case the resulting model will serve as our pilot for our self-driving car.\n",
    "\n",
    "# UPDATES: \n",
    "# - New graphical user interface during training\n",
    "# - Model loss chart is now saved to models folder as a .png image with the name of the model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e13514-020a-42aa-bfb3-ff77a3f51c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "### Machine Learning Libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "\n",
    "# IPython Libraries for display and widgets\n",
    "import traitlets\n",
    "import ipywidgets\n",
    "import ipywidgets.widgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, Layout\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# Custom dataset object library\n",
    "from xy_dataset import XYDataset\n",
    "\n",
    "# General Libraries \n",
    "import cv2, glob, os, fnmatch, collections\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ipyfilechooser import FileChooser\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "\n",
    "# Jupyter Laboratory Libraries\n",
    "import ipywidgets\n",
    "import traitlets\n",
    "from IPython.display import display\n",
    "\n",
    "# Nvidia library for images\n",
    "from jetcam.utils import bgr8_to_jpeg\n",
    "\n",
    "# Custom plot function\n",
    "def live_plot(data_dict, model_file_name=\"model\", figsize=(7,5), title='Model Error Chart:'):\n",
    "    clear_output(wait=True)\n",
    "    fig1 = plt.figure(figsize=figsize)\n",
    "    for label,data in data_dict.items():\n",
    "        plt.plot(data, label= model_file_name + \".pth\") #label)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('Model Loss x1000')\n",
    "    plt.legend(loc='upper right') # the plot evolves to the right\n",
    "    plt.show();\n",
    "    fig1.savefig(training_chart_file_path)\n",
    "\n",
    "# Data collection for MSE information\n",
    "data = collections.defaultdict(list)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42afe817-8dde-4144-9684-3e825c08f040",
   "metadata": {},
   "source": [
    "### Selecting a Dataset for Training\n",
    "Use the following folder chooser to select the folder where your dataset is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76153cb2-ee2b-4df1-ae57-253a65fc9256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display a FileChooser widget\n",
    "fc = FileChooser('/home/racer_core/Datasets')\n",
    "display(fc)\n",
    "fc.show_only_dirs = True\n",
    "# Change the title (use '' to hide)\n",
    "fc.title = '<b>Choose Dataset for Training</b>'\n",
    "\n",
    "# Sample callback function\n",
    "def change_title(chooser):\n",
    "    chooser.title = '<b>Directory Selected.</b>'\n",
    "\n",
    "# Register callback function\n",
    "fc.register_callback(change_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed46c925-1997-42e1-bced-49ad9d76faf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting Dataset\n",
    "\n",
    "# Output from file chooser\n",
    "DATASET_DIR = fc.selected_path\n",
    "dataset_folder_name = DATASET_DIR.split(\"/\")[-1]\n",
    "\n",
    "\n",
    "# Information about the dataset, number of data points and a listing of the data points.\n",
    "num_files =  len(glob.glob(os.path.join(DATASET_DIR, '*.jpg')))\n",
    "file_list = fnmatch.filter(os.listdir(DATASET_DIR), '*.jpg')\n",
    "if num_files > 0:\n",
    "    print(\"Dataset found!\")\n",
    "    print(\"Number of files found in datadset: \" + str(num_files))\n",
    "elif num_files == 0:\n",
    "  print(\"No data in selected directory, choose again?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b0e66-2ac7-454e-aac7-1b01645d8249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our dataset object. This object parses the file names to get the labels for each datapoint\n",
    "\n",
    "# These transforms adjust the images prior to training to promote robust performance\n",
    "# Note: Some transforms are commented out they are example of possible transforms to use in the future\n",
    "TRANSFORMS = transforms.Compose([\n",
    "    #transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),  # Color Jitter #1\n",
    "    #transforms.ColorJitter(brightness=1.3, hue=.3), # Color Jitter #2\n",
    "    #transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)), # Gaussian Blur #1\n",
    "    #transforms.GaussianBlur(kernel_size=(7), sigma=(0.8)),  # Gaussian Blur #2\n",
    "\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(), # MUST USE\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "Sample_Dataset = XYDataset(DATASET_DIR,TRANSFORMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b14c00-6b78-47e2-9f29-1bdffc28ff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn to split dataset into training and evaluation subsets\n",
    "\n",
    "def train_val_dataset(dataset, val_split=0.20):\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "    datasets = {}\n",
    "    datasets['train'] = Subset(dataset, train_idx)\n",
    "    datasets['evaluate'] = Subset(dataset, val_idx)\n",
    "    return datasets\n",
    "\n",
    "# Both \"Train\" and \"Evaluate\" datasets are within the datasets list\n",
    "datasets = train_val_dataset(Sample_Dataset)\n",
    "print(f\"Number of Data Points in Training Dataset: {len(datasets['train'])}\")\n",
    "print(f\"Number of Data Points in Evaluate Dataset: {len(datasets['evaluate'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cb4ae5-70a9-4c20-8a4b-f88009821bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Dataloaders for both the 'train' and the 'eval' datasets\n",
    "# Here the datasets ('train' and 'evaluate') are input into DataLoaders\n",
    "# DataLoaders deliver the data to the training algorithm when requested.\n",
    "# They deliver the data in 'minibatches' , and reshuffle the data for each epoch\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataloader = DataLoader(datasets['train'], batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(datasets['evaluate'], batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed829b-99ca-4a0b-b188-d7be901f23f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting an example of the data from the train loader\n",
    "\n",
    "# Create a figure for both images\n",
    "fig = plt.figure(figsize= (10, 10))\n",
    "\n",
    "# Create a subplot for the array of images\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "\n",
    "# Get an image, list of annotations and labels from the train dataloader\n",
    "train_image, ann, train_labels = next(iter(train_dataloader))\n",
    "\n",
    "# Getting the label for the image\n",
    "x = train_labels[0].numpy()[0]\n",
    "x = int(224 * (x / 2.0 + 0.5))\n",
    "\n",
    "# Reading the raw image from file\n",
    "file_path = ann['image_path'][0]\n",
    "print(f\"Selected File: {file_path}\")\n",
    "print(f\"Label value (x): {x}\")\n",
    "img = cv2.imread(file_path)\n",
    "\n",
    "# Plotting the raw image w/ label\n",
    "circ = Circle((x,112),15)\n",
    "ax.add_patch(circ)\n",
    "ax.imshow(img)\n",
    "\n",
    "# Getting the transformed image from the dataloader (all images on the\n",
    "train_image = train_image.numpy()[0]\n",
    "train_image = np.moveaxis(train_image, 0, -1)\n",
    "\n",
    "# Plotting the transformed image w/ label\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "circ = Circle((x,112),15)\n",
    "ax.add_patch(circ)\n",
    "ax.imshow(train_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1522cf92-3e0a-47e0-a327-c2485a9c9b1e",
   "metadata": {},
   "source": [
    "## Training the CNN on the Selected Dataset\n",
    "Next, we'll setup the training algorithm for our machine learning model.\n",
    "As we prepare to train our model we need to make choices about the way we'll train it.\n",
    "These choices can impact how long it takes to train the model and the overall accuracy of the model.\n",
    "\n",
    "The user-set parameters of the training algorithm are often called \"Hyper-Parameters\"\n",
    "You can set your hyper parameters below, make sure to track which setting you used for your training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05e9082-26d8-4a69-a472-1aac5657fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Training Hyper Parameters:\n",
    "\n",
    "########## [ACTION REQUIRED] Set name for new machine learning model #################\n",
    "model_file_name = \"Enter Model Name here\"\n",
    "training_notes = \"Write notes about the training here\"\n",
    "# Example training notes: testing new dataset collected on Friday 1/10/25 at the library\n",
    "\n",
    "# Number of training epochs: (25 to 35 is a great range to start with)\n",
    "epochs = 30\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "model_folder = \"/home/racer_core/Models/\"\n",
    "model_file_path = model_folder + model_file_name + \".pth\"\n",
    "training_chart_file_path = model_folder+model_file_name+\".png\"\n",
    "\n",
    "# Model Name check\n",
    "if os.path.isfile(model_file_path):\n",
    "    raise Exception('Sorry, model with same name already exists, choose a different model file name.')\n",
    "\n",
    "# Model Output\n",
    "output_dim = 2\n",
    "# Total number of epochs\n",
    "total_epochs = epochs\n",
    "\n",
    "######### Select a Machine learning model structure (Neural Network) ###########\n",
    "######### Uncomment both the model and the fully connected layer \"model.fc\"\n",
    "\n",
    "# Resnet 18\n",
    "model_name = \"Resnet 18\"\n",
    "model = torchvision.models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "model.fc = torch.nn.Linear(512, output_dim)\n",
    "\n",
    "# Resnet 34\n",
    "#model = torchvision.models.resnet34(pretrained=True)\n",
    "#model.fc = torch.nn.Linear(512, output_dim)\n",
    "\n",
    "# Resnet 50\n",
    "#model_name = \"Resnet 50\"\n",
    "#model = torch.hub.load(\"pytorch/vision\", \"resnet50\", weights=\"IMAGENET1K_V2\")\n",
    "#model.fc = torch.nn.Linear(2048, output_dim)\n",
    "\n",
    "# MobileNet V2\n",
    "#model = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "#model.fc = torch.nn.Linear(2048, output_dim)\n",
    "\n",
    "# MobileNet V3\n",
    "#model_name = \"MobileNet_V3\"\n",
    "#model = torchvision.models.mobilenet_v3_large(pretrained=True)\n",
    "#model.classifier[-1] = torch.nn.Linear(1280, output_dim)\n",
    "\n",
    "# ALEXNET\n",
    "#model = torchvision.models.alexnet(pretrained=True)\n",
    "#model.classifier[-1] = torch.nn.Linear(4096, output_dim)\n",
    "\n",
    "# SQUEEZENET \n",
    "#model = torchvision.models.squeezenet1_1(pretrained=True)\n",
    "#model.classifier[1] = torch.nn.Conv2d(512, output_dim, kernel_size=1)\n",
    "#model.num_classes = len(dataset.categories)\n",
    "\n",
    "# DENSENET 121\n",
    "#model = torchvision.models.densenet121(pretrained=True)\n",
    "#model.classifier = torch.nn.Linear(model.num_features, output_dim)\n",
    "\n",
    "# Save the model structure to use later during optimization\n",
    "model_structure = model\n",
    "\n",
    "# If you wanted to train fewer of the layers (freeze some layers) set\n",
    "# Then we freeze the layers of the model\n",
    "# TODO: Freeze all but final layer\n",
    "freeze_layers = True\n",
    "\n",
    "if freeze_layers == True:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Adding a fully connected layer to the top/head of the model\n",
    "\n",
    "# Model optimizer:\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "#Loading a GPU if available and otherwise a CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d492b19a-208f-4658-9100-82a6ecf1b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_and_evaluation(epochs):\n",
    "    # Setting up a simple user interface for the training process\n",
    "    epoch_index_display = widgets.IntText(value=epochs, description='Epoch: ', disabled=False)\n",
    "    epoch_progress_display = widgets.FloatProgress(value=0.0,max=len(datasets['train']) // BATCH_SIZE +1 , description='Epoch Progress:', bar_style='info', style={'bar_color': '#808080'}, orientation='horizontal')\n",
    "    training_error_display = widgets.FloatText(value=0.0, description='Mean Square Error (MSE):', disabled=False) \n",
    "    progress_meter = widgets.FloatProgress(value=0.0, min=0, max=total_epochs, description='Progress:', bar_style='info', style={'bar_color': '#40E0D0'}, orientation='horizontal')\n",
    "    system_status = widgets.Text(value='system_status', placeholder='system_status', description='Status:',disabled=False)\n",
    "    epoch_display = widgets.HBox([epoch_index_display, epoch_progress_display])\n",
    "    training_display = widgets.HBox([training_error_display, progress_meter])\n",
    "    display(epoch_display)\n",
    "    display(training_display)\n",
    "    display(system_status)\n",
    "    out1 = widgets.Output()\n",
    "\n",
    "    display(widgets.HBox([out1]))\n",
    "\n",
    "\n",
    "    # Training Timing\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # Writing training details to training log\n",
    "    f = open(\"/home/racer_core/Models/training_log.txt\", \"a\")\n",
    "    f.write(\"\\n\")\n",
    "    dt_string = start_time.strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "    f.write(f\"Training Report: {dt_string} \\n\")\n",
    "    f.write(f\"Output Model File: {model_file_name}\\n\")\n",
    "    f.write(f\"Selected Dataset: {dataset_folder_name}, Number Data Points: {num_files}\\n\")\n",
    "    f.write(f\"Model: {model_name}, Epochs: {epochs}, Batch Size: {BATCH_SIZE}\\n\")\n",
    "    f.write(f\"Training Notes: {training_notes}\\n\")\n",
    "    f.write(f\"Transforms used in this training: {TRANSFORMS}\\n\")\n",
    "    \n",
    "    ############# Initiating Training Process ##############\n",
    "    # First set model to train mode\n",
    "    model.train()\n",
    "\n",
    "    system_status.value = \"Starting training process ...\"\n",
    "    \n",
    "    # Start training process dependent on number of epochs\n",
    "    while epochs > 0:\n",
    "        system_status.value = \"Training ...\"\n",
    "        # Index\n",
    "        i = 0\n",
    "        sum_loss = 0.0\n",
    "        error_count = 0.0\n",
    "\n",
    "        # Training Loop\n",
    "        # Process each batch of data points in the train loader\n",
    "        for images, category_idx, xy in iter(train_dataloader):\n",
    "            i = i + 1\n",
    "\n",
    "            # send data to device\n",
    "            images = images.to(device)\n",
    "            xy = xy.to(device)\n",
    "            \n",
    "            # zero gradients of parameters\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # execute model to get outputs\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # run backpropogation to accumulate gradients\n",
    "            loss = 0.0\n",
    "            loss += torch.mean((outputs - xy)**2)\n",
    "            loss.backward()\n",
    "            \n",
    "            # step optimizer to adjust parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update Epoch progress meter:\n",
    "            epoch_progress_display.value = i\n",
    "            \n",
    "            # compute MSE loss over x, y coordinates for associated categories\n",
    "            test = False\n",
    "            if test == True:\n",
    "                    \n",
    "                xy = xy.cpu()\n",
    "                outputs = outputs.detach().cpu().numpy().flatten()\n",
    "                \n",
    "                MSE = 0.0\n",
    "                for j in range(len(xy)):\n",
    "                  x = (224*(xy[j].numpy()[0]/2.0 + 0.5))\n",
    "                  xi = (224*(outputs[j]/2.0 + 0.5))\n",
    "                  MSE = MSE + (x-xi)**2\n",
    "                \n",
    "                MSE = MSE/len(xy)\n",
    "                #training_error_display.value = MSE\n",
    "    \n",
    "\n",
    "        \n",
    "        # Evaluation Loop\n",
    "        system_status.value = \"Evaluating ...\"\n",
    "        evaluation_loss = 0.0\n",
    "        i = 0 \n",
    "        for images, category_idx, xy in test_dataloader: # TODO: Make sure that xy is not a tensor\n",
    "        \n",
    "            # Put the model into evaluation mode\n",
    "            model.eval()\n",
    "        \n",
    "            # send data to device\n",
    "            images = images.to(device)\n",
    "            xy = xy.to(device)\n",
    "        \n",
    "            # execute model to get outputs\n",
    "            outputs = model(images)\n",
    "        \n",
    "            # compute MSE loss over x, y coordinates for associated categories\n",
    "            outputs = model(images)\n",
    "            \n",
    "            #MSE = 0.0\n",
    "            #for i in range(len(xy)):\n",
    "            #    x = (224*(xy[i].numpy()[0]/2.0 + 0.5))\n",
    "            #    xi = (224*(outputs[i]/2.0 + 0.5))\n",
    "            #    MSE = MSE + (x-xi)**2\n",
    "\n",
    "                        \n",
    "            loss = 0.0\n",
    "            loss += torch.mean((outputs - xy)**2)\n",
    "            i += len(xy)\n",
    "            evaluation_loss += float(loss)*1000\n",
    "\n",
    "        \n",
    "        #MSE = MSE/len(xy)\n",
    "        #evaluation_loss = MSE\n",
    "        print(f\"Epoch: {epochs}, Evaluation Loss: {evaluation_loss/i}\")\n",
    "        training_error_display.value = evaluation_loss/i\n",
    "\n",
    "        # Update Plot\n",
    "        with out1:\n",
    "            data['MSE'].append(evaluation_loss/i)\n",
    "            live_plot(data, model_file_name=model_file_name, title='Model Error Chart: ' + model_file_name + \".pth\")\n",
    "            \n",
    "        # End of the current epoch\n",
    "        epochs = epochs -1\n",
    "        epoch_index_display.value = epochs\n",
    "        progress_meter.value = (total_epochs - epochs)\n",
    "\n",
    "    \n",
    "    # get the execution time\n",
    "    end_time = datetime.now()\n",
    "    elapsed_time = end_time - start_time\n",
    "    training_duration_time_formatted = str(elapsed_time)\n",
    "    print('Execution time:', training_duration_time_formatted)\n",
    "    \n",
    "    # Finish writing to model training log\n",
    "    f.write(f\"Final model evaluation loss: {evaluation_loss/i}\\n\")\n",
    "    f.write(f\"Total training & evaluation time: {training_duration_time_formatted}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.close()\n",
    "    system_status.value = \"Training complete.\"   \n",
    "    return model #trainLoss, validationLoss, model\n",
    "\n",
    "# START TRAINING\n",
    "model = training_and_evaluation(epochs)\n",
    "\n",
    "# SAVE THE MODEL TO FILE\n",
    "torch.save(model.state_dict(), model_file_path)\n",
    "print(f\"Saved new model as: {model_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67c3464-96c2-43a3-b5cd-94364c76a3c3",
   "metadata": {},
   "source": [
    "### Optimizing the Machine Learning Model to Run on the Robot\n",
    "In this final step to the training process, we'll optimize the model.\n",
    "We optimize the model so that it will run as fast as our camera collects data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e7210b-7184-4630-ad19-9703dc099cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warm starting the new model to be optimized - loading weights from trained model into untrained model\n",
    "model = model_structure # this is the shape of the model before training\n",
    "model = model.cuda().eval().half()\n",
    "model.load_state_dict(torch.load(model_file_path, weights_only=True))\n",
    "\n",
    "# When executed, we should see, \"<All keys matched successfully>\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b98a3bf-dbe9-4ae3-a484-2cf5dd8becae",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Optimization of the Network ############\n",
    "### This step can take a few minutes or longer depending on the size of the mdoel\n",
    "\n",
    "# Custom library from Nvidia to accelerate inference\n",
    "from torch2trt import torch2trt\n",
    "\n",
    "# Example structure of the input data\n",
    "data = torch.zeros((1, 3, 224, 224)).cuda().half()\n",
    "\n",
    "# Model optimization via quantitization, or the reduction of overall model size by reducing the representation of model weights.\n",
    "model_trt = torch2trt(model, [data], fp16_mode=True)\n",
    "\n",
    "# Saving our new optimized model to disk\n",
    "optimized_model_folder = \"/home/racer_core/Models/trt/\"\n",
    "optimized_model_file_path = optimized_model_folder + model_file_name + \"_TRT.pth\"\n",
    "torch.save(model_trt.state_dict(), optimized_model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9901bcf-3cda-4366-84b1-edeafd3bfdf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
